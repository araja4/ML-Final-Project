{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# COMP 755 Machine Learning Final Project\n",
    "By Abu-Bakar Raja and Duy Nguyen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Description of the project:\n",
    "\n",
    "In this project, we have a data set of jeopardy questions and we will classify these questions by their categories. We will first prepare and manipulate the data and discuss the process of vectorizing the data. Then, we will train a basic Multinomial Naive Bayes model using Sklearn and then train an SVM model with Grid Search to find the optimal parameters. Finally, we will compare the performance of these models and try to change the categories we are classifying to improve our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions in this data set:\n",
      "216930\n"
     ]
    }
   ],
   "source": [
    "#importing data and printing size of the data\n",
    "import json\n",
    "\n",
    "with open('JEOPARDY_QUESTIONS1.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "print(\"Number of questions in this data set:\")    \n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Exploring the data \n",
    "First, we want to see how many questions are in each of the categories and how many different categories are there in the data set. We built a dictionary of all the categories and the number of questions in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct categories: \n",
      "27995\n"
     ]
    }
   ],
   "source": [
    "#counting how many questions are there per category and printing how many different category there are in the data set\n",
    "data_info = {}\n",
    "for d in data:\n",
    "    if d['category'] in data_info:\n",
    "        data_info[d['category']] +=1\n",
    "    else:\n",
    "        data_info[d['category']] = 1\n",
    "print('Number of distinct categories: ')\n",
    "print(len(data_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 27,995 categories, we will only try to analyze a subset of these categories to make things more manageable. We will start out with just analyzing the questions under the top 5 most popular categories. Here we get these top 5 categories and how many questions are in each of these 5 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 categories and how many questions are in each:\n",
      "SCIENCE 519\n",
      "AMERICAN HISTORY 418\n",
      "POTPOURRI 401\n",
      "LITERATURE 496\n",
      "BEFORE & AFTER 547\n",
      "\n",
      "number of catagories:  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SCIENCE', 'AMERICAN HISTORY', 'POTPOURRI', 'LITERATURE', 'BEFORE & AFTER']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only using categories that have at least 400 questions aka top 5 categories.\n",
    "#printing the # of useful categories\n",
    "#printing the target_name array\n",
    "\n",
    "print('The top 5 categories and how many questions are in each:')\n",
    "\n",
    "min_num_questions = 400 #use 400 for real data\n",
    "target_names = []\n",
    "num_cat = 0;\n",
    "\n",
    "for x in data_info:\n",
    "    if data_info[x]>= min_num_questions: \n",
    "        print(x,data_info[x])\n",
    "        target_names.append(x)\n",
    "        num_cat +=1\n",
    "\n",
    "print (\"\\nnumber of catagories: \", num_cat)\n",
    "target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now split the data up into train data and keep a small set for testing. We will use 75% of our data to train our models and use the remaining 25% for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the training data set:  1788\n",
      "Number of samples in the test data set:  593\n"
     ]
    }
   ],
   "source": [
    "#spliting the data into train_data and test_data\n",
    "#3/4 of the data is used for traning and 1/4 is used for testing\n",
    "\n",
    "train_data = {'data':[],\n",
    "             'target_names':target_names,\n",
    "            'target': []}\n",
    "test_data = {'data':[],\n",
    "             'target_names':target_names,\n",
    "            'target': []}\n",
    "\n",
    "q_in_cat = []\n",
    "for x in range (len(target_names)):\n",
    "    q_in_cat.append(0)\n",
    "\n",
    "for d in data:\n",
    "    if d['category'] in target_names:\n",
    "        # put every forth question in data into our test data set \n",
    "        if q_in_cat[target_names.index(d['category'])] % 4 == 3:\n",
    "            test_data['data'].append(d['question'])\n",
    "            test_data['target'].append(target_names.index(d['category']))\n",
    "        else:\n",
    "            train_data['data'].append(d['question'])\n",
    "            train_data['target'].append(target_names.index(d['category']))\n",
    "        q_in_cat[target_names.index(d['category'])] +=1\n",
    "        \n",
    "print('Number of samples in the training data set: ', len(train_data['data']))\n",
    "print('Number of samples in the test data set: ', len(test_data['data']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data set prepared, we need to extract features for each sample from the data set by vectorizing the text of these questions into numerical values that we can do analysis on. There are many ways to process text and extract features out of them ranging from a simple bag of words approach to using a neural net such as Word2Vec. Each way has their own benefits and use cases, but here, we will take the most naive and intuitive approach and use the simple bag of words representation to process these texts. \n",
    "\n",
    "\n",
    "As the name suggests, we will \"put all of our words into a bag\" where we will assign a unique integer ID to each distinct word occuring within our train data set, and then for each sample i, we will count the number of occurences of the word w in the sample and store that in the i vector at w's ID. Therefore, the number of features is the number of distinct words that we have in the data set and for each feature (or word), we have the number of occurences in that sample. \n",
    "\n",
    "We note that the questions we have in the data set are fairly short in length compared to the number of distinct words in our data set. Only a tiny fracture of all the possible words in our bag will appear in each entry, and thus, we will end up with a lot of 0's in our feature vectors. Therefore, we will only store the non-zeros features for each sample.\n",
    "\n",
    "We can do this using sklearn's CountVectorizer from their feature_extraction library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1788, 7006)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train_data['data'])\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train_counts is now our feature matrix with 1788 of our training samples and we can see that our \"bag\" has 7006 words in total. The words in the bag and their ID's are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'at': 828,\n",
       " 'sea': 5648,\n",
       " 'level': 3833,\n",
       " '70': 450,\n",
       " 'degrees': 2028,\n",
       " 'this': 6383,\n",
       " 'travels': 6511,\n",
       " '129': 54,\n",
       " 'feet': 2632,\n",
       " 'per': 4793,\n",
       " 'second': 5660,\n",
       " 'it': 3536,\n",
       " 'speeds': 5983,\n",
       " 'up': 6630,\n",
       " 'over': 4650,\n",
       " 'foot': 2748,\n",
       " 'sec': 5659,\n",
       " 'for': 2752,\n",
       " 'each': 2292,\n",
       " 'rising': 5442,\n",
       " 'degree': 2027,\n",
       " 'the': 6354,\n",
       " 'largest': 3750,\n",
       " 'tree': 6520,\n",
       " 'general': 2902,\n",
       " 'sherman': 5766,\n",
       " 'in': 3411,\n",
       " 'california': 1312,\n",
       " 'is': 3524,\n",
       " 'type': 6584,\n",
       " 'also': 661,\n",
       " 'called': 1314,\n",
       " 'sierra': 5809,\n",
       " 'redwood': 5288,\n",
       " 'href': 3332,\n",
       " 'http': 3333,\n",
       " 'www': 6964,\n",
       " 'archive': 767,\n",
       " 'com': 1652,\n",
       " 'media': 4135,\n",
       " '2006': 355,\n",
       " '02': 5,\n",
       " '06_dj_13': 24,\n",
       " 'jpg': 3629,\n",
       " 'target': 6281,\n",
       " '_blank': 476,\n",
       " 'sarah': 5576,\n",
       " 'of': 4543,\n",
       " 'clue': 1600,\n",
       " 'crew': 1865,\n",
       " 'reads': 5261,\n",
       " 'from': 2831,\n",
       " 'pole': 4945,\n",
       " 'vault': 6673,\n",
       " 'duke': 2276,\n",
       " 'university': 6616,\n",
       " 'track': 6479,\n",
       " 'durham': 2282,\n",
       " 'nc': 4410,\n",
       " 'bending': 1017,\n",
       " 'an': 691,\n",
       " 'elastic': 2353,\n",
       " 'solid': 5929,\n",
       " 'stress': 6120,\n",
       " 'force': 2754,\n",
       " 'causing': 1409,\n",
       " 'deformation': 2025,\n",
       " 'letter': 3829,\n",
       " 'term': 6329,\n",
       " '06_dj_13a': 25,\n",
       " '06_dj_15': 26,\n",
       " 'wmv': 6917,\n",
       " 'honey': 3286,\n",
       " 'colored': 1645,\n",
       " 'retriever': 5392,\n",
       " 'named': 4378,\n",
       " 'max': 4103,\n",
       " 'tries': 6529,\n",
       " 'to': 6431,\n",
       " 'lick': 3845,\n",
       " 'cheryl': 1503,\n",
       " 'as': 805,\n",
       " 'she': 5752,\n",
       " 'pets': 4823,\n",
       " 'him': 3250,\n",
       " 'state': 6051,\n",
       " 'raleigh': 5218,\n",
       " 'veterinarians': 6698,\n",
       " 'refer': 5290,\n",
       " 'area': 770,\n",
       " 'animal': 710,\n",
       " 'body': 1109,\n",
       " 'posterior': 4989,\n",
       " 'or': 4594,\n",
       " 'region': 5303,\n",
       " 'latin': 3761,\n",
       " 'tail': 6261,\n",
       " 'on': 4571,\n",
       " 'may': 4106,\n",
       " '29': 396,\n",
       " '1765': 128,\n",
       " 'patrick': 4745,\n",
       " 'henry': 3209,\n",
       " 'stamp': 6038,\n",
       " 'act': 531,\n",
       " 'protest': 5118,\n",
       " 'was': 6792,\n",
       " 'interrupted': 3496,\n",
       " 'with': 6908,\n",
       " 'one': 4573,\n",
       " 'word': 6933,\n",
       " 'some': 5933,\n",
       " 'think': 6380,\n",
       " 'irving': 3523,\n",
       " 'berlin': 1027,\n",
       " 'song': 5939,\n",
       " 'should': 5790,\n",
       " 'replace': 5338,\n",
       " 'star': 6045,\n",
       " 'spangled': 5969,\n",
       " 'banner': 921,\n",
       " 'national': 4396,\n",
       " 'anthem': 723,\n",
       " 'easier': 2304,\n",
       " 'sing': 5831,\n",
       " 'politicians': 4953,\n",
       " 'often': 4556,\n",
       " 'complain': 1689,\n",
       " 'about': 493,\n",
       " 'having': 3157,\n",
       " 'make': 4007,\n",
       " 'appearances': 745,\n",
       " 'unappetizing': 6591,\n",
       " 'poultry': 5000,\n",
       " 'circuit': 1545,\n",
       " 'there': 6366,\n",
       " 'are': 769,\n",
       " 'blue': 1098,\n",
       " 'white': 6860,\n",
       " 'branches': 1169,\n",
       " 'african': 581,\n",
       " 'river': 5446,\n",
       " 'website': 6819,\n",
       " 'bureau': 1266,\n",
       " 'treasury': 6515,\n",
       " 'department': 2050,\n",
       " 'moneyfactory': 4276,\n",
       " 'became': 975,\n",
       " 'territory': 6335,\n",
       " '1900': 241,\n",
       " '59': 437,\n",
       " 'years': 6977,\n",
       " 'later': 3759,\n",
       " 'susan': 6217,\n",
       " 'anthony': 726,\n",
       " 'arrested': 788,\n",
       " '1872': 214,\n",
       " 'doing': 2200,\n",
       " 'april': 756,\n",
       " '1984': 328,\n",
       " 'government': 2996,\n",
       " 'agency': 589,\n",
       " 'admitted': 566,\n",
       " 'its': 3540,\n",
       " 'role': 5473,\n",
       " 'mining': 4230,\n",
       " 'nicaraguan': 4454,\n",
       " 'harbors': 3127,\n",
       " 'john': 3611,\n",
       " 'hancock': 3105,\n",
       " 'held': 3193,\n",
       " 'political': 4950,\n",
       " 'position': 4982,\n",
       " '1780': 137,\n",
       " '85': 464,\n",
       " '1787': 140,\n",
       " '93': 471,\n",
       " 'december': 1999,\n",
       " '1974': 317,\n",
       " 'former': 2769,\n",
       " 'new': 4439,\n",
       " 'york': 6988,\n",
       " 'governor': 2997,\n",
       " 'sworn': 6243,\n",
       " 'vice': 6702,\n",
       " 'president': 5039,\n",
       " 'party': 4727,\n",
       " 'founded': 2784,\n",
       " 'around': 783,\n",
       " '1789': 142,\n",
       " 'stood': 6097,\n",
       " 'strong': 6128,\n",
       " 'central': 1433,\n",
       " 'his': 3255,\n",
       " 'foes': 2735,\n",
       " 'said': 5543,\n",
       " 'that': 6353,\n",
       " '1877': 219,\n",
       " 'he': 3168,\n",
       " 'agreed': 597,\n",
       " 'withdraw': 6909,\n",
       " 'remaining': 5324,\n",
       " 'federal': 2628,\n",
       " 'troops': 6539,\n",
       " 'south': 5959,\n",
       " 'return': 5394,\n",
       " 'electoral': 2359,\n",
       " 'support': 6193,\n",
       " '1917': 258,\n",
       " 'purchased': 5162,\n",
       " 'islands': 3529,\n",
       " 'st': 6027,\n",
       " 'croix': 1878,\n",
       " 'thomas': 6385,\n",
       " 'country': 1827,\n",
       " '25': 376,\n",
       " 'million': 4217,\n",
       " 'when': 6851,\n",
       " 'west': 6839,\n",
       " 'virginia': 6727,\n",
       " '1863': 204,\n",
       " 'wheeling': 6849,\n",
       " 'capital': 1342,\n",
       " 'city': 1557,\n",
       " 'permanent': 4810,\n",
       " '1885': 225,\n",
       " '1698': 108,\n",
       " 'after': 583,\n",
       " 'absence': 497,\n",
       " '15': 81,\n",
       " 'returned': 5395,\n",
       " 'colony': 1642,\n",
       " 'father': 2604,\n",
       " 'aug': 855,\n",
       " '1826': 171,\n",
       " 'boston': 1141,\n",
       " 'faneuil': 2589,\n",
       " 'hall': 3093,\n",
       " 'great': 3025,\n",
       " 'orator': 4596,\n",
       " 'delivered': 2034,\n",
       " 'eulogy': 2485,\n",
       " 'jefferson': 3578,\n",
       " 'adams': 545,\n",
       " 'metric': 4183,\n",
       " 'measurement': 4128,\n",
       " '10': 39,\n",
       " 'millimeters': 4216,\n",
       " 'equal': 2436,\n",
       " 'these': 6370,\n",
       " 'boyle': 1162,\n",
       " 'law': 3769,\n",
       " 'says': 5598,\n",
       " 'normally': 4490,\n",
       " 'if': 3380,\n",
       " 'you': 6990,\n",
       " 'double': 2220,\n",
       " 'pressure': 5043,\n",
       " 'gas': 2885,\n",
       " 'volume': 6746,\n",
       " 'decreases': 2009,\n",
       " 'by': 1293,\n",
       " 'amount': 684,\n",
       " 'mole': 4265,\n",
       " 'any': 735,\n",
       " 'substance': 6152,\n",
       " 'always': 668,\n",
       " 'has': 3144,\n",
       " 'same': 5558,\n",
       " 'number': 4512,\n",
       " '022': 6,\n",
       " 'sup': 6184,\n",
       " '23': 373,\n",
       " 'resin': 5371,\n",
       " 'natural': 4402,\n",
       " 'polymer': 4957,\n",
       " 'used': 6642,\n",
       " 'varnish': 6671,\n",
       " 'produced': 5082,\n",
       " 'insects': 3470,\n",
       " 'india': 3428,\n",
       " 'and': 696,\n",
       " 'myanmar': 4361,\n",
       " 'between': 1037,\n",
       " '1856': 197,\n",
       " '1860': 201,\n",
       " '962': 473,\n",
       " 'faith': 2576,\n",
       " 'set': 5722,\n",
       " 'out': 4636,\n",
       " 'iowa': 3515,\n",
       " 'nebraska': 4414,\n",
       " 'utah': 6646,\n",
       " 'handcart': 3107,\n",
       " 'migration': 4200,\n",
       " 'wilson': 6890,\n",
       " 'told': 6441,\n",
       " 'congress': 1728,\n",
       " 'world': 6941,\n",
       " 'must': 4355,\n",
       " 'be': 961,\n",
       " 'made': 3978,\n",
       " 'safe': 5540,\n",
       " '20': 347,\n",
       " '1971': 314,\n",
       " 'supreme': 6199,\n",
       " 'court': 1833,\n",
       " 'upheld': 6632,\n",
       " 'transportation': 6506,\n",
       " 'method': 4182,\n",
       " 'way': 6806,\n",
       " 'achieve': 518,\n",
       " 'school': 5616,\n",
       " 'integration': 3487,\n",
       " '1798': 146,\n",
       " 'passed': 4731,\n",
       " 'collection': 1630,\n",
       " 'bills': 1052,\n",
       " 'control': 1764,\n",
       " 'domestic': 2209,\n",
       " 'dissent': 2169,\n",
       " 'conspiracy': 1742,\n",
       " 'against': 586,\n",
       " 'govt': 2999,\n",
       " 'chapter': 1467,\n",
       " '52': 433,\n",
       " 'novel': 4504,\n",
       " 'boisterous': 1115,\n",
       " 'crowd': 1888,\n",
       " 'gathering': 2889,\n",
       " 'fagin': 2569,\n",
       " 'execution': 2516,\n",
       " '19th': 345,\n",
       " 'author': 862,\n",
       " 'known': 3710,\n",
       " 'writing': 6957,\n",
       " 'venerable': 6683,\n",
       " 'mansion': 4031,\n",
       " 'seven': 5727,\n",
       " 'acutely': 543,\n",
       " 'peaked': 4766,\n",
       " 'gables': 2857,\n",
       " 'people': 4791,\n",
       " 'past': 4736,\n",
       " 'appear': 743,\n",
       " 'brother': 1217,\n",
       " 'sister': 5846,\n",
       " 'rewards': 5411,\n",
       " 'fairies': 2573,\n",
       " 'jungle': 3643,\n",
       " 'book': 1129,\n",
       " 'even': 2493,\n",
       " 'grendel': 3038,\n",
       " 'would': 6947,\n",
       " 'love': 3937,\n",
       " 'seamus': 5654,\n",
       " 'heaney': 3175,\n",
       " 'translation': 6503,\n",
       " 'anglo': 708,\n",
       " 'saxon': 5594,\n",
       " 'epic': 2432,\n",
       " 'title': 6428,\n",
       " 'geat': 2897,\n",
       " '1980': 323,\n",
       " 'scarefest': 5606,\n",
       " 'which': 6854,\n",
       " 'mom': 4270,\n",
       " 'daughter': 1968,\n",
       " 'switch': 6241,\n",
       " 'bodies': 1108,\n",
       " 'day': 1974,\n",
       " 'stalked': 6033,\n",
       " 'jason': 3567,\n",
       " 'camp': 1325,\n",
       " 'crystal': 1900,\n",
       " 'lake': 3734,\n",
       " 'leif': 3811,\n",
       " 'ericson': 2451,\n",
       " 'dad': 1933,\n",
       " 'who': 6864,\n",
       " 'huge': 3338,\n",
       " 'low': 3944,\n",
       " 'surface': 6203,\n",
       " 'temperature': 6317,\n",
       " 'fictional': 2658,\n",
       " 'girl': 2939,\n",
       " 'sleuth': 5878,\n",
       " 'granddaughter': 3012,\n",
       " 'profile': 5088,\n",
       " 'lethal': 3828,\n",
       " 'weapon': 6812,\n",
       " 'director': 2143,\n",
       " 'whose': 6871,\n",
       " 'group': 3048,\n",
       " 'caught': 1405,\n",
       " 'nevada': 4437,\n",
       " 'pass': 4729,\n",
       " 'winter': 6901,\n",
       " '1846': 187,\n",
       " '47': 426,\n",
       " 'ellen': 2377,\n",
       " 'glasgow': 2948,\n",
       " 'native': 4399,\n",
       " 'several': 5729,\n",
       " 'novels': 4507,\n",
       " 'but': 1284,\n",
       " 'queenborough': 5185,\n",
       " 'character': 1469,\n",
       " 'why': 6872,\n",
       " 'did': 2110,\n",
       " 'paint': 4680,\n",
       " 'will': 6885,\n",
       " 'mock': 4258,\n",
       " 'me': 4118,\n",
       " 'horribly': 3302,\n",
       " 'madame': 3977,\n",
       " 'bovary': 1155,\n",
       " 'visited': 6734,\n",
       " 'tunisia': 6563,\n",
       " 'research': 5362,\n",
       " 'salammbo': 5548,\n",
       " 'carthage': 1381,\n",
       " 'eugenie': 2484,\n",
       " 'grandet': 3013,\n",
       " 'considered': 1739,\n",
       " 'finest': 2683,\n",
       " 'series': 5711,\n",
       " 'la': 3721,\n",
       " 'comedie': 1658,\n",
       " 'humaine': 3343,\n",
       " 'scott': 5638,\n",
       " 'fitzgerald': 2700,\n",
       " 'classic': 1567,\n",
       " 'nick': 4456,\n",
       " 'carraway': 1371,\n",
       " 'lives': 3888,\n",
       " 'next': 4447,\n",
       " 'door': 2216,\n",
       " 'daisy': 1940,\n",
       " 'miller': 4214,\n",
       " 'opens': 4581,\n",
       " 'trois': 6538,\n",
       " 'couronnes': 1831,\n",
       " 'hotel': 3316,\n",
       " 'vevey': 6699,\n",
       " 'switzerland': 6242,\n",
       " 'published': 5146,\n",
       " 'third': 6382,\n",
       " 'cool': 1777,\n",
       " '1934': 276,\n",
       " 'year': 6975,\n",
       " 'miss': 4245,\n",
       " 'lonelyhearts': 3908,\n",
       " 'english': 2415,\n",
       " 'ivan': 3544,\n",
       " 'turgenev': 6564,\n",
       " 'ottsy': 4634,\n",
       " 'deti': 2086,\n",
       " 'familial': 2584,\n",
       " 'thin': 6376,\n",
       " 'piece': 4856,\n",
       " 'disputed': 2167,\n",
       " 'israeli': 3533,\n",
       " 'palestinian': 4691,\n",
       " 'land': 3739,\n",
       " 'involved': 3509,\n",
       " 'clothes': 1594,\n",
       " 'shedding': 5753,\n",
       " 'card': 1356,\n",
       " 'game': 2865,\n",
       " 'film': 2674,\n",
       " 'legend': 3804,\n",
       " '1823': 168,\n",
       " 'edict': 2325,\n",
       " 'european': 2488,\n",
       " 'intervention': 3498,\n",
       " 'western': 6840,\n",
       " 'hemisphere': 3201,\n",
       " 'robert': 5455,\n",
       " 'lee': 3799,\n",
       " 'right': 5430,\n",
       " 'arm': 777,\n",
       " 'sang': 5571,\n",
       " 'abc': 481,\n",
       " 'singing': 5834,\n",
       " '1935': 277,\n",
       " 'lunar': 3962,\n",
       " 'florida': 2727,\n",
       " 'turned': 6568,\n",
       " 'into': 3499,\n",
       " '80s': 458,\n",
       " 'cop': 1781,\n",
       " 'show': 5792,\n",
       " '2001': 350,\n",
       " 'sweden': 6231,\n",
       " 'honored': 3290,\n",
       " 'award': 875,\n",
       " '100th': 41,\n",
       " 'anniversary': 716,\n",
       " 'postage': 4987,\n",
       " 'stamps': 6040,\n",
       " 'select': 5683,\n",
       " 'comfort': 1663,\n",
       " 'corporation': 1803,\n",
       " 'makes': 4010,\n",
       " 'adjustable': 558,\n",
       " 'firmness': 2691,\n",
       " '2003': 352,\n",
       " 'nationwide': 4398,\n",
       " 'free': 2806,\n",
       " 'slurpee': 5887,\n",
       " 'date': 1964,\n",
       " 'flower': 2729,\n",
       " 'got': 2986,\n",
       " 'name': 4377,\n",
       " 'belief': 1004,\n",
       " 'bees': 989,\n",
       " 'sweet': 6234,\n",
       " 'charles': 1475,\n",
       " 'schulz': 5624,\n",
       " 'snoopy': 5905,\n",
       " 'didn': 2111,\n",
       " 'become': 979,\n",
       " 'lead': 3776,\n",
       " 'until': 6625,\n",
       " 'began': 992,\n",
       " 'walking': 6773,\n",
       " 'brit': 1201,\n",
       " 'can': 1330,\n",
       " 'tell': 6313,\n",
       " 'liverpudlian': 3887,\n",
       " 'persil': 4813,\n",
       " 'french': 2812,\n",
       " 'ever': 2499,\n",
       " 'popular': 4969,\n",
       " 'garnish': 2880,\n",
       " 'conference': 1722,\n",
       " 'two': 6582,\n",
       " 'religious': 5320,\n",
       " 'groups': 3050,\n",
       " 'sponsors': 6012,\n",
       " 'brotherhood': 1218,\n",
       " 'sisterhood': 5847,\n",
       " 'week': 6824,\n",
       " 'first': 2692,\n",
       " 'fairy': 2575,\n",
       " 'tales': 6270,\n",
       " '1835': 178,\n",
       " 'pamphlet': 4695,\n",
       " 'tinder': 6420,\n",
       " 'box': 1158,\n",
       " 'among': 682,\n",
       " 'them': 6360,\n",
       " 'nicodemus': 4460,\n",
       " 'frapp': 2802,\n",
       " 'narrow': 4388,\n",
       " 'minded': 4223,\n",
       " 'evangelist': 2492,\n",
       " 'tono': 6453,\n",
       " 'bungay': 1262,\n",
       " '1909': 249,\n",
       " 'time': 6417,\n",
       " 'machine': 3975,\n",
       " 'good': 2980,\n",
       " 'earth': 2300,\n",
       " 'based': 941,\n",
       " 'heroine': 3224,\n",
       " 'her': 3213,\n",
       " '1938': 279,\n",
       " 'proud': 5123,\n",
       " 'heart': 3178,\n",
       " 'herself': 3225,\n",
       " 'electromagnetic': 2365,\n",
       " 'rays': 5246,\n",
       " 'take': 6264,\n",
       " 'pictures': 4855,\n",
       " 'your': 6993,\n",
       " 'insides': 3472,\n",
       " 'were': 6837,\n",
       " 'originally': 4617,\n",
       " 'roentgen': 5468,\n",
       " 'combined': 1654,\n",
       " 'oxygen': 4664,\n",
       " 'lightest': 3855,\n",
       " 'chemical': 1497,\n",
       " 'element': 2370,\n",
       " 'water': 6798,\n",
       " 'someone': 5934,\n",
       " 'warsaw': 6791,\n",
       " 'strongest': 6129,\n",
       " 'points': 4937,\n",
       " 'magnetic': 3988,\n",
       " 'field': 2660,\n",
       " 'symbol': 6247,\n",
       " 'radioactive': 5206,\n",
       " 'pu': 5142,\n",
       " 'sounds': 5955,\n",
       " 'like': 3858,\n",
       " 'mickey': 4191,\n",
       " 'mouse': 4316,\n",
       " 'dog': 2197,\n",
       " 'british': 1204,\n",
       " 'commander': 1667,\n",
       " 'sir': 5844,\n",
       " 'edward': 2333,\n",
       " 'pakenham': 4688,\n",
       " 'killed': 3685,\n",
       " 'battle': 954,\n",
       " 'fought': 2782,\n",
       " 'weeks': 6826,\n",
       " 'war': 6783,\n",
       " '1812': 160,\n",
       " 'reach': 5248,\n",
       " 'eastern': 2307,\n",
       " 'markets': 4054,\n",
       " '1800s': 152,\n",
       " 'texas': 6344,\n",
       " 'drovers': 2259,\n",
       " 'brought': 1220,\n",
       " 'their': 6358,\n",
       " 'cattle': 1404,\n",
       " 'kansas': 3651,\n",
       " 'via': 6701,\n",
       " 'trail': 6492,\n",
       " 'mar': 4039,\n",
       " '27': 387,\n",
       " '1964': 306,\n",
       " 'alaska': 614,\n",
       " 'hit': 3259,\n",
       " 'earthquake': 2301,\n",
       " 'arthur': 799,\n",
       " 'clair': 1562,\n",
       " 'vast': 6672,\n",
       " 'north': 4492,\n",
       " 'ohio': 4558,\n",
       " 'tai': 6260,\n",
       " 'pan': 4696,\n",
       " 'hong': 3287,\n",
       " 'kong': 3714,\n",
       " 'james': 3555,\n",
       " 'clavell': 1572,\n",
       " '1975': 318,\n",
       " 'japan': 3562,\n",
       " 'pierre': 4858,\n",
       " 'boulle': 1151,\n",
       " 'planet': 4896,\n",
       " 'apes': 738,\n",
       " 'zira': 7003,\n",
       " 'cornelius': 1796,\n",
       " 'species': 5977,\n",
       " 'ape': 737,\n",
       " 'milan': 4201,\n",
       " 'kundera': 3719,\n",
       " 'immortality': 3393,\n",
       " 'read': 5254,\n",
       " 'original': 4616,\n",
       " 'language': 3743,\n",
       " 'unbearably': 6594,\n",
       " 'light': 3853,\n",
       " 'reading': 5258,\n",
       " 'completes': 1692,\n",
       " 'eldridge': 2354,\n",
       " 'cleaver': 1576,\n",
       " '1968': 310,\n",
       " 'memoir': 4151,\n",
       " 'soul': 5952,\n",
       " 'singer': 5832,\n",
       " 'my': 4360,\n",
       " 'cherie': 1500,\n",
       " 'amour': 685,\n",
       " 'secret': 5662,\n",
       " 'identity': 3378,\n",
       " 'diana': 2106,\n",
       " 'prince': 5057,\n",
       " 'exorcist': 2525,\n",
       " 'disappears': 2146,\n",
       " 'maryland': 4077,\n",
       " 'woods': 6929,\n",
       " 'scary': 5607,\n",
       " '1999': 344,\n",
       " 'louisa': 3934,\n",
       " 'alcott': 620,\n",
       " 'relationship': 5313,\n",
       " 'guru': 3075,\n",
       " 'gray': 3022,\n",
       " 'collaborated': 1623,\n",
       " 'sequel': 5706,\n",
       " 'dystopian': 2291,\n",
       " 'burgess': 1269,\n",
       " 'zealand': 6996,\n",
       " 'fish': 2693,\n",
       " 'walt': 6776,\n",
       " 'whitman': 6862,\n",
       " 'section': 5664,\n",
       " 'myself': 4363,\n",
       " 'longest': 3911,\n",
       " 'work': 6936,\n",
       " '1855': 196,\n",
       " 'steinbeck': 6069,\n",
       " 'lennie': 3815,\n",
       " 'nightmarish': 4467,\n",
       " 'visions': 6732,\n",
       " 'dead': 1983,\n",
       " 'aunt': 858,\n",
       " 'clara': 1563,\n",
       " 'gigantic': 2931,\n",
       " 'rabbit': 5198,\n",
       " 'cain': 1304,\n",
       " 'been': 987,\n",
       " 'filmed': 2675,\n",
       " 'twice': 6578,\n",
       " 'written': 6958,\n",
       " 'under': 6600,\n",
       " 'bar': 923,\n",
       " 'muriel': 4344,\n",
       " 'spark': 5972,\n",
       " 'marcia': 4041,\n",
       " 'blaine': 1078,\n",
       " 'girls': 2941,\n",
       " 'edinburgh': 2326,\n",
       " 'tale': 6268,\n",
       " 'cities': 1551,\n",
       " 'dr': 2232,\n",
       " 'alexander': 629,\n",
       " 'manette': 4024,\n",
       " 'released': 5316,\n",
       " '18': 150,\n",
       " 'prison': 5068,\n",
       " 'through': 6398,\n",
       " 'looking': 3919,\n",
       " 'glass': 2949,\n",
       " 'humpty': 3349,\n",
       " 'dumpty': 2278,\n",
       " 'explains': 2536,\n",
       " 'alice': 637,\n",
       " 'meaning': 4123,\n",
       " 'nonsense': 4487,\n",
       " 'poem': 4930,\n",
       " 'final': 2678,\n",
       " 'scene': 5608,\n",
       " 'rebecca': 5267,\n",
       " 'stately': 6053,\n",
       " 'maxim': 4104,\n",
       " 'de': 1981,\n",
       " 'burns': 1276,\n",
       " '1965': 307,\n",
       " 'frank': 2799,\n",
       " 'herbert': 3215,\n",
       " 'refers': 5293,\n",
       " 'desert': 2069,\n",
       " 'arrakis': 785,\n",
       " 'otis': 4633,\n",
       " 'redding': 5285,\n",
       " 'no': 4476,\n",
       " 'performed': 4802,\n",
       " 'scottish': 5639,\n",
       " 'band': 914,\n",
       " 'night': 4465,\n",
       " 'rhett': 5414,\n",
       " 'butler': 1286,\n",
       " 'tells': 6315,\n",
       " 'toad': 6432,\n",
       " 'rat': 5232,\n",
       " 'dear': 1991,\n",
       " 'don': 2210,\n",
       " 'give': 2942,\n",
       " 'damn': 1944,\n",
       " 'beatles': 970,\n",
       " '1967': 309,\n",
       " 'album': 617,\n",
       " 'wwii': 6963,\n",
       " 'miniseries': 4231,\n",
       " 'hbo': 3167,\n",
       " 'sidney': 5807,\n",
       " 'poitier': 4940,\n",
       " 'movie': 4324,\n",
       " 'opera': 4582,\n",
       " 'preceded': 5016,\n",
       " 'mamie': 4016,\n",
       " 'eisenhower': 2350,\n",
       " 'lady': 3730,\n",
       " 'wizard': 6915,\n",
       " 'oz': 4666,\n",
       " 'big': 1045,\n",
       " 'screen': 5640,\n",
       " 'tom': 6445,\n",
       " 'hanks': 3115,\n",
       " 'man': 4020,\n",
       " 'irish': 3520,\n",
       " 'mob': 4255,\n",
       " 'liberator': 3839,\n",
       " 'king': 3692,\n",
       " 'scotland': 5636,\n",
       " 'starred': 6046,\n",
       " 'die': 2112,\n",
       " 'hard': 3128,\n",
       " 'american': 678,\n",
       " 'negotiated': 4425,\n",
       " 'treaty': 6519,\n",
       " 'greenville': 3033,\n",
       " 'earned': 2298,\n",
       " 'nickname': 4458,\n",
       " 'mr': 4330,\n",
       " 'las': 3753,\n",
       " 'vegas': 6677,\n",
       " 'danke': 1953,\n",
       " 'schoen': 5615,\n",
       " 'using': 6644,\n",
       " '200': 348,\n",
       " 'old': 4562,\n",
       " 'technique': 6301,\n",
       " 'hospital': 3309,\n",
       " 'long': 3910,\n",
       " 'beach': 962,\n",
       " 'calif': 1311,\n",
       " 'utilized': 6649,\n",
       " 'fly': 2732,\n",
       " 'larvae': 3752,\n",
       " 'treat': 6516,\n",
       " 'patients': 4744,\n",
       " 'ranking': 5227,\n",
       " 'just': 3646,\n",
       " 'above': 494,\n",
       " 'captain': 1345,\n",
       " 'not': 4498,\n",
       " 'behind': 1000,\n",
       " 'navy': 4407,\n",
       " 'admiral': 562,\n",
       " '1989': 333,\n",
       " 'pope': 4968,\n",
       " 'paul': 4751,\n",
       " 'ii': 3383,\n",
       " 'added': 549,\n",
       " 'automobile': 869,\n",
       " 'related': 5311,\n",
       " 'acts': 541,\n",
       " 'official': 4553,\n",
       " 'list': 3876,\n",
       " 'catholic': 1402,\n",
       " 'sins': 5842,\n",
       " 'culture': 1910,\n",
       " 'club': 1598,\n",
       " 'seal': 5651,\n",
       " 'smooching': 5897,\n",
       " 'bullet': 1255,\n",
       " 'head': 3169,\n",
       " 'heavy': 3187,\n",
       " 'metal': 4175,\n",
       " 'rap': 5228,\n",
       " 'perfected': 4800,\n",
       " 'richard': 5422,\n",
       " 'gatling': 2890,\n",
       " 'late': 3758,\n",
       " 'century': 1435,\n",
       " 'chaucer': 1489,\n",
       " 'adapted': 547,\n",
       " '1990s': 335,\n",
       " 'tv': 6573,\n",
       " 'sci': 5625,\n",
       " 'fi': 2654,\n",
       " 'horror': 3304,\n",
       " 'anthology': 725,\n",
       " 'melville': 4146,\n",
       " 'artificial': 802,\n",
       " 'leg': 3801,\n",
       " 'carved': 1385,\n",
       " 'jawbone': 3570,\n",
       " 'sperm': 5986,\n",
       " 'whale': 6842,\n",
       " 'jim': 3598,\n",
       " 'hawkins': 3162,\n",
       " 'narrates': 4386,\n",
       " '1883': 223,\n",
       " 'start': 6049,\n",
       " 'dickens': 2108,\n",
       " 'pip': 4876,\n",
       " 'meets': 4142,\n",
       " 'escaped': 2465,\n",
       " 'convict': 1770,\n",
       " 'threatens': 6395,\n",
       " 'eat': 2309,\n",
       " 'nearly': 4413,\n",
       " 'sucrose': 6159,\n",
       " 'fruit': 2836,\n",
       " 'sugar': 6166,\n",
       " 'main': 3999,\n",
       " 'sweetener': 6235,\n",
       " 'archipelago': 765,\n",
       " 'ring': 5434,\n",
       " 'coral': 1790,\n",
       " 'bikini': 1048,\n",
       " 'pacific': 4672,\n",
       " 'developed': 2090,\n",
       " 'jenner': 3580,\n",
       " 'true': 6545,\n",
       " 'vaccine': 6652,\n",
       " 'designed': 2074,\n",
       " 'protect': 5113,\n",
       " 'disease': 2156,\n",
       " 'milne': 4221,\n",
       " 'boy': 1160,\n",
       " 'stole': 6092,\n",
       " 'rich': 5421,\n",
       " 'poor': 4966,\n",
       " 'instructor': 3483,\n",
       " 'favorite': 2613,\n",
       " 'end': 2402,\n",
       " 'exclamation': 2512,\n",
       " 'point': 4935,\n",
       " 'played': 4908,\n",
       " 'luke': 3959,\n",
       " 'skywalker': 5866,\n",
       " 'dogpatch': 2198,\n",
       " 'resident': 5368,\n",
       " 'claims': 1561,\n",
       " 'have': 3155,\n",
       " 'invented': 3503,\n",
       " 'baseball': 940,\n",
       " 'statue': 6058,\n",
       " 'torch': 6461,\n",
       " '1986': 330,\n",
       " '100': 40,\n",
       " 'sunshine': 6183,\n",
       " 'once': 4572,\n",
       " 'had': 3083,\n",
       " 'east': 2305,\n",
       " 'portions': 4975,\n",
       " 'part': 4720,\n",
       " 'now': 4509,\n",
       " 'belongs': 1012,\n",
       " 'ala': 611,\n",
       " 'thermometer': 6369,\n",
       " 'developer': 2091,\n",
       " 'discovered': 2149,\n",
       " 'boiling': 1114,\n",
       " 'liquid': 3874,\n",
       " 'varies': 6668,\n",
       " 'atmospheric': 833,\n",
       " 'noble': 4478,\n",
       " 'gases': 2886,\n",
       " 'lowest': 3948,\n",
       " 'atomic': 835,\n",
       " 'weight': 6829,\n",
       " '1838': 180,\n",
       " '1841': 183,\n",
       " 'naturalist': 4404,\n",
       " 'secretary': 5663,\n",
       " 'geological': 2912,\n",
       " 'society': 5917,\n",
       " 'london': 3906,\n",
       " 'kelp': 3668,\n",
       " 'especially': 2469,\n",
       " 'source': 5956,\n",
       " 'halogen': 3096,\n",
       " 'kleptophobia': 3702,\n",
       " 'fear': 2617,\n",
       " 'wanted': 6782,\n",
       " 'do': 2188,\n",
       " 'all': 640,\n",
       " 'kleptomaniac': 3701,\n",
       " '1803': 154,\n",
       " 'emperor': 2395,\n",
       " 'youngest': 6992,\n",
       " 'jerome': 3585,\n",
       " 'married': 4061,\n",
       " 'teenager': 6307,\n",
       " 'baltimore': 910,\n",
       " 'discoveryland': 2152,\n",
       " 'euro': 2486,\n",
       " 'disney': 2160,\n",
       " 'equivalent': 2445,\n",
       " 'futuristic': 2855,\n",
       " 'found': 2783,\n",
       " 'disneyland': 2161,\n",
       " 'museita': 4348,\n",
       " 'waltz': 6778,\n",
       " 'highlight': 3245,\n",
       " 'composer': 1696,\n",
       " '1896': 233,\n",
       " 'boheme': 1111,\n",
       " 'toronto': 6466,\n",
       " 'freaks': 2804,\n",
       " '815': 459,\n",
       " 'landmark': 3740,\n",
       " 'becomes': 980,\n",
       " 'medieval': 4138,\n",
       " 'fortress': 2777,\n",
       " 'bank': 918,\n",
       " 'thames': 6348,\n",
       " 'garp': 2881,\n",
       " 'literary': 3880,\n",
       " 'creator': 1856,\n",
       " 'goes': 2964,\n",
       " 'songwriting': 5942,\n",
       " 'god': 2961,\n",
       " 'bless': 1087,\n",
       " 'america': 677,\n",
       " 'schoolhouse': 5618,\n",
       " 'rock': 5461,\n",
       " 'making': 4011,\n",
       " ...}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(count_vect.vocabulary_))\n",
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using just the number of occurences, we can improve this and use the frequency of appearance of each word to account for longer, more wordy questions asking about the same thing. These longer questions have the same content as their shorter counterparts, but they will have a higher number of occurences since they are longer. Therefore, by switching to using frequency and dividing the number of occurences of each word in the sample by the total number of words in each sample, we can normalize these values.\n",
    "\n",
    "Moreover, words that appear a lot across different samples are not as informative as those who appears less frequently. Therefore, we will also want to scale down the weights of these words that appear frequently across samples.\n",
    "\n",
    "We use sklearn's tfdif to transform the feature matrix we got before to account for the frequency. Our new matrix will now have the frequency of each words in each sample instead of just the count and the weights for popular words in the data set will be scaled down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1788, 7006)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model to classify quesitons\n",
    "Now that we have our features matrix, we can train a classifer to predict the categories of questions. We will start with a simple multinomial Naive Bayes model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, train_data['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clf is now our trained model and we can do a quick check to see if it works and what the model would predict for some questions. Just as a quick check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The element with the atomic number of 2' => SCIENCE\n",
      "'The war that ended slavery' => AMERICAN HISTORY\n",
      "'The author of the popular youth series Harry Potter' => LITERATURE\n",
      "'The year slavery was abolished' => AMERICAN HISTORY\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['The element with the atomic number of 2', 'The war that ended slavery', 'The author of the popular youth series Harry Potter', 'The year slavery was abolished']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, train_data['target_names'][category]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a working model, we want to use it to predict the categories for questions in our test data set to see how well we did. We composed everything from above into a pipeline so that we can reuse it quickly. In this pipeline, we will use the simple count vectorizer from above to get the feature matrix of the number of occurences, then we normalize the matrix with respect to the length of each sample with tfdif, and finally, we train our Naive Bayes model on that vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "clf_wo_normalizing = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('clf', MultinomialNB())])\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB())])\n",
    "\n",
    "clf_wo_normalizing = clf_wo_normalizing.fit(train_data['data'], train_data['target'])\n",
    "text_clf = text_clf.fit(train_data['data'], train_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model with normalzing for length: 0.7217537942664418\n",
      "Accuracy of the model without normalzing for length: 0.7234401349072512\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "predicted = text_clf.predict(test_data['data'])\n",
    "acc_with = np.mean(predicted == test_data['target'])\n",
    "print('Accuracy of the model with normalzing for length:', acc_with)\n",
    "\n",
    "predicted = clf_wo_normalizing.predict(test_data['data'])\n",
    "acc_without = np.mean(predicted == test_data['target'])\n",
    "print('Accuracy of the model without normalzing for length:', acc_without)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of the model used and the result\n",
    "\n",
    "##### Overall Accuracy:\n",
    "We got about 72% accuracy with just a simple Naive Bayes model. For a start, this is not bad, but only 72% is not a good enough accuracy. This is due to the fact that some of the categories have overlapping content with the others. In fact, Potpourri, by definition, is a mixture of things, and therefore, the questions in this category could easily have been anything, including being a history question or a literature question or a science question. This means that some of our categories to classify were subsets of one of the other categories, thus, there was a whole lot of overlap as one question could have been easily in both Potpourri AND science. Below, as we improve our project, we will train with questions and categories that have less overlap.\n",
    "\n",
    "##### Normalizing vs Not Normalizing:\n",
    "We actually got a lower accuracy with normalizing for length. This is because these questions are Jeopardy questions, and thus, they have relatively the same length, leading to little to no difference between normalizing and not. However, since the values are close enough, we will just keep the normalizing step in case we ever need to extend this to apply to a different data set. \n",
    "\n",
    "\n",
    "### Try a different model\n",
    "Now let's see if we can do better with a different text classifying algorithm. Here we will try the linear support vector machine algorithm, which is a popular text classification algorithm. The biggest difference between SVM and Multinomial Naive Bayes (MNB) is that MNB treats the features as independent while SVM looks for interaction between features. Thus, SVM is theoretically better for when there are dependencies among the features since SVM can take into account these dependencies whereas MNB cannot. Given our data set, we want to see if SVM would be better than a MNB since our data contains shorter questions, and the words in the questions may or may not have any relationship. This is motivated by a paper that suggests MNB is better than SVM for shorter snippets (https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf) and since we have really short questions that resembles snippets a little bit, we want to test this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM: 0.7082630691399663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-5, n_iter=5, random_state=42)),\n",
    "])\n",
    "_ = text_clf_svm.fit(train_data['data'], train_data['target'])\n",
    "predicted_svm = text_clf_svm.predict(test_data['data'])\n",
    "svm_acc = np.mean(predicted_svm == test_data['target'])\n",
    "print('Accuracy of SVM:', svm_acc )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a lover accuracy score with our SVM model at 70% compared to the 72% of our Naive Bayes model. While it is not a definite conclusion, this might be due to the fact these questions are, in fact, just snippets and this, there was not a relationship or dependency among the features and that's why MNB did better, even though not by much.\n",
    "\n",
    "### Grid Search to fine tune the parameters\n",
    "\n",
    "Instead of exhaustively plugging all the possible parameters to optimize our model, we can use grid search to look for the optimized parameters. For example, below we are considering unigrams and bigrams and trying different alpha values for the penalty term. We can set up values for the parameters that we want to try and grid search can do it for us. (We set n_jobs to -1 so that we can use all of the detected cores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    ">>> from sklearn.model_selection import GridSearchCV\n",
    ">>> parameters = {\n",
    "...     'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "...     'tfidf__use_idf': (True, False),\n",
    "...     'clf__alpha': (1e-2, 1e-3),\n",
    "... }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for grid search:  0.6979573183865223\n"
     ]
    }
   ],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, iid=False, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(train_data['data'], train_data['target'])\n",
    "print('Best score for grid search: ', gs_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"RUNNING stuff top 3 without 'POTPOURRI' and 'BEFORE & AFTER'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running with jsut the top 3 categories WITHOUT Potpourri and Before and After\n",
    "\n",
    "In order to address a problem we had before with one of our categories potentially being a subset one another, we will remove Potpourri and Before and After since we suspect that since Potpourri and Before and After can be any question, they are confusing our MNB model. Therefore, we will carry out the same steps as above but only with 3 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCIENCE 519\n",
      "AMERICAN HISTORY 418\n",
      "LITERATURE 496\n",
      "\n",
      "number of catagories:  3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SCIENCE', 'AMERICAN HISTORY', 'LITERATURE']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only using 'SCIENCE', 'AMERICAN HISTORY', 'LITERATURE' as the categories aka removing 'POTPOURRI' and 'BEFORE & AFTER' since they overlap\n",
    "#printing the # categories\n",
    "\n",
    "target_names = ['SCIENCE', 'AMERICAN HISTORY', 'LITERATURE']\n",
    "num_cat = 0;\n",
    "\n",
    "for x in target_names:\n",
    "    print(x,data_info[x])\n",
    "    num_cat +=1\n",
    "\n",
    "print (\"\\nnumber of catagories: \", num_cat)\n",
    "target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#spliting the data into train_data and test_data\n",
    "#3/4 of the data is used for traning and 1/4 is used for testing\n",
    "\n",
    "train_data = {'data':[],\n",
    "             'target_names':target_names,\n",
    "            'target': []}\n",
    "test_data = {'data':[],\n",
    "             'target_names':target_names,\n",
    "            'target': []}\n",
    "\n",
    "q_in_cat = []\n",
    "for x in range (len(target_names)):\n",
    "    q_in_cat.append(0)\n",
    "\n",
    "for d in data:\n",
    "    if d['category'] in target_names:\n",
    "        if q_in_cat[target_names.index(d['category'])] % 4 == 3:\n",
    "            test_data['data'].append(d['question'])\n",
    "            test_data['target'].append(target_names.index(d['category']))\n",
    "        else:\n",
    "            train_data['data'].append(d['question'])\n",
    "            train_data['target'].append(target_names.index(d['category']))\n",
    "        q_in_cat[target_names.index(d['category'])] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1076, 4776)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train_data['data'])\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1076, 4776)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, train_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The element with the atomic number of 2' => SCIENCE\n",
      "'The war that ended slavery' => AMERICAN HISTORY\n",
      "'The author of the popular youth series Harry Potter' => LITERATURE\n",
      "'The year slavery was abolished' => AMERICAN HISTORY\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['The element with the atomic number of 2', 'The war that ended slavery', 'The author of the popular youth series Harry Potter', 'The year slavery was abolished']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, train_data['target_names'][category]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB())])\n",
    "\n",
    "text_clf = text_clf.fit(train_data['data'], train_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8935574229691877"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf.predict(test_data['data'])\n",
    "np.mean(predicted == test_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8935574229691877"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                   ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-5, n_iter=5, random_state=42)),\n",
    " ])\n",
    "_ = text_clf_svm.fit(train_data['data'], train_data['target'])\n",
    "predicted_svm = text_clf_svm.predict(test_data['data'])\n",
    "np.mean(predicted_svm == test_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "     'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "     'tfidf__use_idf': (True, False),\n",
    "     'clf__alpha': (1e-2, 1e-3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for grid search:  0.888470139341689\n"
     ]
    }
   ],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, iid=False, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(train_data['data'], train_data['target'])\n",
    "print('Best score for grid search: ', gs_clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of the result of running just Science, American History, and Literature\n",
    "\n",
    "By removing the two problematic categories, we have greatly increased our accuracy score to 89% across all models. This confirms our earlier suspicion that Potpourri and Before and After contained questions that could have as easily been in any of the categories. These 3 categories have a clearer and more easily defined differences such that our model can now draw a clearer line to classify them. \n",
    "\n",
    "Knowing that the model will perform a whole lot better if we are trying to classify categories with as little overlap as possible, we will now try to do the whole process again with the 5 categories being Science, American History, Literature, Sports, and Business and Industry. We believe that this is a more reasonable set of categories to try to classify questions into since these are distinguishable and none of them is a subset of another by definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCIENCE 519\n",
      "AMERICAN HISTORY 418\n",
      "LITERATURE 496\n",
      "SPORTS 342\n",
      "BUSINESS & INDUSTRY 311\n",
      "\n",
      "number of catagories:  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SCIENCE', 'AMERICAN HISTORY', 'LITERATURE', 'SPORTS', 'BUSINESS & INDUSTRY']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only using 'SCIENCE', 'AMERICAN HISTORY', 'LITERATURE','SPORTS', 'BUSINESS & INDUSTRY' as the categories aka 5 distinct categories\n",
    "#printing the # categories\n",
    "\n",
    "target_names = ['SCIENCE', 'AMERICAN HISTORY', 'LITERATURE', 'SPORTS', 'BUSINESS & INDUSTRY']\n",
    "num_cat = 0;\n",
    "\n",
    "for x in target_names:\n",
    "    print(x,data_info[x])\n",
    "    num_cat +=1\n",
    "\n",
    "print (\"\\nnumber of catagories: \", num_cat)\n",
    "target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#spliting the data into train_data and test_data\n",
    "#3/4 of the data is used for traning and 1/4 is used for testing\n",
    "\n",
    "train_data = {'data':[],\n",
    "             'target_names':target_names,\n",
    "            'target': []}\n",
    "test_data = {'data':[],\n",
    "             'target_names':target_names,\n",
    "            'target': []}\n",
    "\n",
    "q_in_cat = []\n",
    "for x in range (len(target_names)):\n",
    "    q_in_cat.append(0)\n",
    "\n",
    "for d in data:\n",
    "    if d['category'] in target_names:\n",
    "        if q_in_cat[target_names.index(d['category'])] % 4 == 3:\n",
    "            test_data['data'].append(d['question'])\n",
    "            test_data['target'].append(target_names.index(d['category']))\n",
    "        else:\n",
    "            train_data['data'].append(d['question'])\n",
    "            train_data['target'].append(target_names.index(d['category']))\n",
    "        q_in_cat[target_names.index(d['category'])] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1567, 6135)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train_data['data'])\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1567, 6135)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, train_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The element with the atomic number of 2' => SCIENCE\n",
      "'The war that ended slavery' => AMERICAN HISTORY\n",
      "'The author of the popular youth series Harry Potter' => LITERATURE\n",
      "'The year slavery was abolished' => AMERICAN HISTORY\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['The element with the atomic number of 2', 'The war that ended slavery', 'The author of the popular youth series Harry Potter', 'The year slavery was abolished']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, train_data['target_names'][category]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB())])\n",
    "\n",
    "text_clf = text_clf.fit(train_data['data'], train_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.815028901734104"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf.predict(test_data['data'])\n",
    "np.mean(predicted == test_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8439306358381503"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from sklearn.linear_model import SGDClassifier\n",
    ">>> text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "...                      ('tfidf', TfidfTransformer()),\n",
    "...                      ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "...                                            alpha=1e-5, n_iter=5, random_state=42)),\n",
    "... ])\n",
    ">>> _ = text_clf_svm.fit(train_data['data'], train_data['target'])\n",
    ">>> predicted_svm = text_clf_svm.predict(test_data['data'])\n",
    ">>> np.mean(predicted_svm == test_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    ">>> from sklearn.model_selection import GridSearchCV\n",
    ">>> parameters = {\n",
    "...     'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "...     'tfidf__use_idf': (True, False),\n",
    "...     'clf__alpha': (1e-2, 1e-3),\n",
    "... }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8474518169999097"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, iid=False, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(train_data['data'], train_data['target'])\n",
    "gs_clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Discussion of the result of running on 5 different (or so we thought) categories\n",
    "\n",
    "We obtained an accuracy of about 84% when running on the 5 hand-selected categories. Again, we picked these categories because we think that they are different enough for a decent accuracy score, and we got a pretty decent score as predicted. We only did ever slightly worse than when we ran only with 3 categories, which makes sense since we are adding more categories to classify. This shows that we need to formulate our problem carefully to ensure that there is little overlapping among the target groups and that no group is a subgroup of another since that will definitely confuse the MNB model. \n",
    "\n",
    "### Future Work\n",
    "\n",
    "For future work, it will be interesting to try more text classification models on this data set and compare the models similarly to what we did here with MNB and SVM. There are so many models out there to try and we need to analyze our data set more rigoriously to choose what model we should use to fit the data. \n",
    "\n",
    "It will also be interesting to try out different ways of extracting the features and see which method makes the most sense for our data set. Maybe we can try Word2Vec or other extraction algorithms.\n",
    "\n",
    "We can also extend this work and applied our findings and experiences to a more pressing problem of classifying Piazza questions for students (like proposed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Clustering (For fun)\n",
    "\n",
    "Here we did a quick clustering analysis to ee "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.5,\n",
    "                                 min_df=2, stop_words='english',\n",
    "                                 use_idf= True)\n",
    "\n",
    "X = vectorizer.fit_transform(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=20, init='k-means++', max_iter=100, n_init=1,\n",
    "                verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=20, n_init=1, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.351\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "labels = twenty_train.target\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0: uiuc cso baseball year players illinois game article team cs\n",
      "Cluster 1: windows window mouse dos motif server problem com using screen\n",
      "Cluster 2: car access digex com cars pat ax radar engine usa\n",
      "Cluster 3: people israel israeli com article don think just jews government\n",
      "Cluster 4: sale 00 offer shipping condition new price asking 10 university\n",
      "Cluster 5: com netcom hp article posting sun ibm nntp host stratus\n",
      "Cluster 6: nasa gov space jpl larc baalke gsfc jsc higgins center\n",
      "Cluster 7: gun guns firearms people weapons com militia don amendment control\n",
      "Cluster 8: ca canada university bc columbia bnr usc posting host nntp\n",
      "Cluster 9: team game hockey ca nhl play games players season toronto\n",
      "Cluster 10: keith caltech livesey sgi solntze wpd jon schneider cco morality\n",
      "Cluster 11: drive scsi ide controller drives hard disk bus floppy hd\n",
      "Cluster 12: god jesus christians bible people christian christ faith believe christianity\n",
      "Cluster 13: turkish armenian armenians armenia argic turks serdar turkey zuma genocide\n",
      "Cluster 14: university posting host nntp thanks know like cs article just\n",
      "Cluster 15: uk ac henry toronto alaska zoo spencer zoology university aurora\n",
      "Cluster 16: georgia ai uga gatech prism covington mcovingt athens michael aisun3\n",
      "Cluster 17: key clipper encryption chip keys escrow government com crypto nsa\n",
      "Cluster 18: file files windows ___ format bmp program ini cview image\n",
      "Cluster 19: geb banks gordon pitt cs dsl n3jxp chastity cadre shameful\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(20):\n",
    "        print(\"Cluster %d:\" % i, end='')\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind], end='')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
