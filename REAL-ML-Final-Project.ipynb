{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP 755 Machine Learning Final Project\n",
    "By Abu-Bakar Raja and Duy Nguyen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the project:\n",
    "\n",
    "This project blah blah jeopardy questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216930\n"
     ]
    }
   ],
   "source": [
    "#importing data and printing size of the data\n",
    "import json\n",
    "\n",
    "with open('JEOPARDY_QUESTIONS1.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27995"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#counting how many questions are there per category and printing how many different category there are in the data set\n",
    "data_info = {}\n",
    "for d in data:\n",
    "    if d['category'] in data_info:\n",
    "        data_info[d['category']] +=1\n",
    "    else:\n",
    "        data_info[d['category']] = 1\n",
    "len(data_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'RUNNING stuff for top 5 categories'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCIENCE 519\n",
      "AMERICAN HISTORY 418\n",
      "POTPOURRI 401\n",
      "LITERATURE 496\n",
      "BEFORE & AFTER 547\n",
      "\n",
      "number of catagories:  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SCIENCE', 'AMERICAN HISTORY', 'POTPOURRI', 'LITERATURE', 'BEFORE & AFTER']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only using categories that have at least 400 questions aka top 5 categories.\n",
    "#printing the # of useful categories\n",
    "#printing the target_name array\n",
    "\n",
    "min_num_questions = 400 #use 400 for real data\n",
    "target_names = []\n",
    "num_cat = 0;\n",
    "\n",
    "for x in data_info:\n",
    "    if data_info[x]>= min_num_questions: \n",
    "        print(x,data_info[x])\n",
    "        target_names.append(x)\n",
    "        num_cat +=1\n",
    "\n",
    "print (\"\\nnumber of catagories: \", num_cat)\n",
    "target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#spliting the data into train_data and test_data\n",
    "#3/4 of the data is used for traning and 1/4 is used for testing\n",
    "\n",
    "train_data = {'data':[],\n",
    "             'target_names':target_names,\n",
    "            'target': []}\n",
    "test_data = {'data':[],\n",
    "             'target_names':target_names,\n",
    "            'target': []}\n",
    "\n",
    "q_in_cat = []\n",
    "for x in range (len(target_names)):\n",
    "    q_in_cat.append(0)\n",
    "\n",
    "for d in data:\n",
    "    if d['category'] in target_names:\n",
    "        if q_in_cat[target_names.index(d['category'])] % 4 == 3:\n",
    "            test_data['data'].append(d['question'])\n",
    "            test_data['target'].append(target_names.index(d['category']))\n",
    "        else:\n",
    "            train_data['data'].append(d['question'])\n",
    "            train_data['target'].append(target_names.index(d['category']))\n",
    "        q_in_cat[target_names.index(d['category'])] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1788, 7006)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train_data['data'])\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1788, 7006)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, train_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => BEFORE & AFTER\n",
      "'We need to work out more' => SCIENCE\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['God is love', 'We need to work out more']\n",
    ">>> X_new_counts = count_vect.transform(docs_new)\n",
    ">>> X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    ">>> predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    ">>> for doc, category in zip(docs_new, predicted):\n",
    "...     print('%r => %s' % (doc, train_data['target_names'][category]))\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB())])\n",
    "\n",
    "text_clf = text_clf.fit(train_data['data'], train_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7217537942664418"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf.predict(test_data['data'])\n",
    "np.mean(predicted == test_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aburaja\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7082630691399663"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from sklearn.linear_model import SGDClassifier\n",
    ">>> text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "...                      ('tfidf', TfidfTransformer()),\n",
    "...                      ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "...                                            alpha=1e-5, n_iter=5, random_state=42)),\n",
    "... ])\n",
    ">>> _ = text_clf_svm.fit(train_data['data'], train_data['target'])\n",
    ">>> predicted_svm = text_clf_svm.predict(test_data['data'])\n",
    ">>> np.mean(predicted_svm == test_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    ">>> from sklearn.model_selection import GridSearchCV\n",
    ">>> parameters = {\n",
    "...     'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "...     'tfidf__use_idf': (True, False),\n",
    "...     'clf__alpha': (1e-2, 1e-3),\n",
    "... }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, iid=False, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gs_clf = gs_clf.fit(train_data['data'], train_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6979573183865223"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"RUNNING stuff top 3 without 'POTPOURRI' and 'BEFORE & AFTER'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCIENCE 519\n",
      "AMERICAN HISTORY 418\n",
      "LITERATURE 496\n",
      "\n",
      "number of catagories:  3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SCIENCE', 'AMERICAN HISTORY', 'LITERATURE']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only using 'SCIENCE', 'AMERICAN HISTORY', 'LITERATURE' as the categories aka removing 'POTPOURRI' and 'BEFORE & AFTER' since they overlap\n",
    "#printing the # categories\n",
    "\n",
    "target_names = ['SCIENCE', 'AMERICAN HISTORY', 'LITERATURE']\n",
    "num_cat = 0;\n",
    "\n",
    "for x in target_names:\n",
    "    print(x,data_info[x])\n",
    "    num_cat +=1\n",
    "\n",
    "print (\"\\nnumber of catagories: \", num_cat)\n",
    "target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#spliting the data into train_data and test_data\n",
    "#3/4 of the data is used for traning and 1/4 is used for testing\n",
    "\n",
    "train_data = {'data':[],\n",
    "             'target_names':target_names,\n",
    "            'target': []}\n",
    "test_data = {'data':[],\n",
    "             'target_names':target_names,\n",
    "            'target': []}\n",
    "\n",
    "q_in_cat = []\n",
    "for x in range (len(target_names)):\n",
    "    q_in_cat.append(0)\n",
    "\n",
    "for d in data:\n",
    "    if d['category'] in target_names:\n",
    "        if q_in_cat[target_names.index(d['category'])] % 4 == 3:\n",
    "            test_data['data'].append(d['question'])\n",
    "            test_data['target'].append(target_names.index(d['category']))\n",
    "        else:\n",
    "            train_data['data'].append(d['question'])\n",
    "            train_data['target'].append(target_names.index(d['category']))\n",
    "        q_in_cat[target_names.index(d['category'])] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1076, 4776)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train_data['data'])\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1076, 4776)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, train_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => LITERATURE\n",
      "'We need to work out more' => SCIENCE\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['God is love', 'We need to work out more']\n",
    ">>> X_new_counts = count_vect.transform(docs_new)\n",
    ">>> X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    ">>> predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    ">>> for doc, category in zip(docs_new, predicted):\n",
    "...     print('%r => %s' % (doc, train_data['target_names'][category]))\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB())])\n",
    "\n",
    "text_clf = text_clf.fit(train_data['data'], train_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8935574229691877"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf.predict(test_data['data'])\n",
    "np.mean(predicted == test_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aburaja\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8935574229691877"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from sklearn.linear_model import SGDClassifier\n",
    ">>> text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "...                      ('tfidf', TfidfTransformer()),\n",
    "...                      ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "...                                            alpha=1e-5, n_iter=5, random_state=42)),\n",
    "... ])\n",
    ">>> _ = text_clf_svm.fit(train_data['data'], train_data['target'])\n",
    ">>> predicted_svm = text_clf_svm.predict(test_data['data'])\n",
    ">>> np.mean(predicted_svm == test_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    ">>> from sklearn.model_selection import GridSearchCV\n",
    ">>> parameters = {\n",
    "...     'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "...     'tfidf__use_idf': (True, False),\n",
    "...     'clf__alpha': (1e-2, 1e-3),\n",
    "... }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, iid=False, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gs_clf = gs_clf.fit(train_data['data'], train_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.888470139341689"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"RUNNING stuff for 'SCIENCE', 'AMERICAN HISTORY', 'LITERATURE','SPORTS', 'BUSINESS & INDUSTRY'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCIENCE 519\n",
      "AMERICAN HISTORY 418\n",
      "LITERATURE 496\n",
      "SPORTS 342\n",
      "BUSINESS & INDUSTRY 311\n",
      "\n",
      "number of catagories:  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SCIENCE', 'AMERICAN HISTORY', 'LITERATURE', 'SPORTS', 'BUSINESS & INDUSTRY']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only using 'SCIENCE', 'AMERICAN HISTORY', 'LITERATURE','SPORTS', 'BUSINESS & INDUSTRY' as the categories aka 5 distinct categories\n",
    "#printing the # categories\n",
    "\n",
    "target_names = ['SCIENCE', 'AMERICAN HISTORY', 'LITERATURE', 'SPORTS', 'BUSINESS & INDUSTRY']\n",
    "num_cat = 0;\n",
    "\n",
    "for x in target_names:\n",
    "    print(x,data_info[x])\n",
    "    num_cat +=1\n",
    "\n",
    "print (\"\\nnumber of catagories: \", num_cat)\n",
    "target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#spliting the data into train_data and test_data\n",
    "#3/4 of the data is used for traning and 1/4 is used for testing\n",
    "\n",
    "train_data = {'data':[],\n",
    "             'target_names':target_names,\n",
    "            'target': []}\n",
    "test_data = {'data':[],\n",
    "             'target_names':target_names,\n",
    "            'target': []}\n",
    "\n",
    "q_in_cat = []\n",
    "for x in range (len(target_names)):\n",
    "    q_in_cat.append(0)\n",
    "\n",
    "for d in data:\n",
    "    if d['category'] in target_names:\n",
    "        if q_in_cat[target_names.index(d['category'])] % 4 == 3:\n",
    "            test_data['data'].append(d['question'])\n",
    "            test_data['target'].append(target_names.index(d['category']))\n",
    "        else:\n",
    "            train_data['data'].append(d['question'])\n",
    "            train_data['target'].append(target_names.index(d['category']))\n",
    "        q_in_cat[target_names.index(d['category'])] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1567, 6135)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train_data['data'])\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "count_vect.vocabulary_.get(u'algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1567, 6135)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, train_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => LITERATURE\n",
      "'We need to work out more' => SCIENCE\n"
     ]
    }
   ],
   "source": [
    "docs_new = ['God is love', 'We need to work out more']\n",
    ">>> X_new_counts = count_vect.transform(docs_new)\n",
    ">>> X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    ">>> predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    ">>> for doc, category in zip(docs_new, predicted):\n",
    "...     print('%r => %s' % (doc, train_data['target_names'][category]))\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                      ('tfidf', TfidfTransformer()),\n",
    "                      ('clf', MultinomialNB())])\n",
    "\n",
    "text_clf = text_clf.fit(train_data['data'], train_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.815028901734104"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf.predict(test_data['data'])\n",
    "np.mean(predicted == test_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aburaja\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8439306358381503"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from sklearn.linear_model import SGDClassifier\n",
    ">>> text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "...                      ('tfidf', TfidfTransformer()),\n",
    "...                      ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "...                                            alpha=1e-5, n_iter=5, random_state=42)),\n",
    "... ])\n",
    ">>> _ = text_clf_svm.fit(train_data['data'], train_data['target'])\n",
    ">>> predicted_svm = text_clf_svm.predict(test_data['data'])\n",
    ">>> np.mean(predicted_svm == test_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    ">>> from sklearn.model_selection import GridSearchCV\n",
    ">>> parameters = {\n",
    "...     'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "...     'tfidf__use_idf': (True, False),\n",
    "...     'clf__alpha': (1e-2, 1e-3),\n",
    "... }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, iid=False, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gs_clf = gs_clf.fit(train_data['data'], train_data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8474518169999097"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"Clustring stuff... below\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.5,\n",
    "                                 min_df=2, stop_words='english',\n",
    "                                 use_idf= True)\n",
    "\n",
    "X = vectorizer.fit_transform(twenty_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=20, init='k-means++', max_iter=100, n_init=1,\n",
    "                verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=20, n_init=1, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.351\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "labels = twenty_train.target\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0: uiuc cso baseball year players illinois game article team cs\n",
      "Cluster 1: windows window mouse dos motif server problem com using screen\n",
      "Cluster 2: car access digex com cars pat ax radar engine usa\n",
      "Cluster 3: people israel israeli com article don think just jews government\n",
      "Cluster 4: sale 00 offer shipping condition new price asking 10 university\n",
      "Cluster 5: com netcom hp article posting sun ibm nntp host stratus\n",
      "Cluster 6: nasa gov space jpl larc baalke gsfc jsc higgins center\n",
      "Cluster 7: gun guns firearms people weapons com militia don amendment control\n",
      "Cluster 8: ca canada university bc columbia bnr usc posting host nntp\n",
      "Cluster 9: team game hockey ca nhl play games players season toronto\n",
      "Cluster 10: keith caltech livesey sgi solntze wpd jon schneider cco morality\n",
      "Cluster 11: drive scsi ide controller drives hard disk bus floppy hd\n",
      "Cluster 12: god jesus christians bible people christian christ faith believe christianity\n",
      "Cluster 13: turkish armenian armenians armenia argic turks serdar turkey zuma genocide\n",
      "Cluster 14: university posting host nntp thanks know like cs article just\n",
      "Cluster 15: uk ac henry toronto alaska zoo spencer zoology university aurora\n",
      "Cluster 16: georgia ai uga gatech prism covington mcovingt athens michael aisun3\n",
      "Cluster 17: key clipper encryption chip keys escrow government com crypto nsa\n",
      "Cluster 18: file files windows ___ format bmp program ini cview image\n",
      "Cluster 19: geb banks gordon pitt cs dsl n3jxp chastity cadre shameful\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(20):\n",
    "        print(\"Cluster %d:\" % i, end='')\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind], end='')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
